{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7681b9d0",
   "metadata": {},
   "source": [
    "### Author Features Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dddac8",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "Classification is probably the most popular task that you would deal with in real life.\n",
    "Text in the form of blogs, posts, articles, etc. is written every second. It is a challenge to predict the information\n",
    "about the writer without knowing about him/her.\n",
    "We are going to create a classifier that predicts multiple features of the author of a given text.\n",
    "We have designed it as a Multilabel classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e6a131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e587672",
   "metadata": {},
   "source": [
    "**1. Load the dataset (5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc91d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/Ramyasridar/Downloads/NLP/project/blogtext.csv',nrows=3000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3dfac00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f9acad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "gender    0\n",
       "age       0\n",
       "topic     0\n",
       "sign      0\n",
       "date      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed61594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'gender', 'age', 'topic', 'sign', 'date', 'text'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050c959",
   "metadata": {},
   "source": [
    "Inference: The original dataset hold 681284 observations and 7 variables.as the dataset is large we are selecting 3000 rows for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927da3a",
   "metadata": {},
   "source": [
    "**2. Preprocess rows of the “text” column (7.5 points)**\n",
    "\n",
    "**a. Remove unwanted characters**\n",
    "\n",
    "**b. Convert text to lowercase**\n",
    "\n",
    "**c. Remove unwanted spaces**\n",
    "\n",
    "**d. Remove stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798ba290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "048508e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231502e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ramyasridar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a66359e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53c79875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>Info has been found (+/- 100 pages,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>These are the team members:   Drewe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>In het kader van kernfusie op aarde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing!!!  testing!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>Thanks to Yahoo!'s Toolbar I can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0             Info has been found (+/- 100 pages,...  \n",
       "1             These are the team members:   Drewe...  \n",
       "2             In het kader van kernfusie op aarde...  \n",
       "3                   testing!!!  testing!!!            \n",
       "4               Thanks to Yahoo!'s Toolbar I can ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8352bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91fb0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de25ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73b5e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "df.text = df.text.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28092739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    info found pages mb pdf files wait untill team...\n",
       "1    team members drewes van der laag urllink mail ...\n",
       "2    het kader van kernfusie op aarde maak je eigen...\n",
       "3                                      testing testing\n",
       "4    thanks yahoo toolbar capture urls popups means...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c0740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interesting conversation dad morning talking koreans put money invariably lot real estate cash cash would include short term investments one year well savings accounts reason real estate makes money lot money seen surveys seoul real estate rising per year long stretches even taking account crisis referred imf crisis although imf bailed korea compare korean corporate bonds fell modestly recovered local stock market represented kospi version dow jones index gone appreciably high points points see urllink link see real estate makes sense back conversation noted real big elite real estate investor billion usd see urllink converter properties dad seemed little flabbergasted heck need million dollars need much retire maybe lot risk take real estate south korean asset example north toots horn louder make move country usd worth cents also denominated imf crisis dropped vis vis usd also make bad investment fall victim scam latest urllink good morning city project toast saw lady tv lost everything comment tears know like go rich person beggar one day one saber rattling north korea weak exchange rate little nest egg could almost wiped government almost zero help unemployed disabled otherwise disenfranchised workers role family important money help family go first thus idea koreans go things investing different one apartment urllink jeonse system supports well see usd apartment rent two systems use korea neither western ones except rare circumstances renter signs year contract deposits half market value usd owner monthly rent paid owner invest korean treasury bills per year monthly rent return end term usd returned renter renter signs year year contract deposits market value property usd plus monthly rent month cases value property increases decreases jeonse need topped partially refunded course using usd save key help foreigners reference better thus buy place turn around rent get like buy another place whatever since mortgages korea kind cash society although home equity lines credit system bit different key course real estate prices keep going'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd0d38",
   "metadata": {},
   "source": [
    "Inference: the data has been preprocessed from the observation of first five rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2e3c0",
   "metadata": {},
   "source": [
    "**3. As we want to make this into a multi-label classification problem, you are required to merge all the label\n",
    "   columns together, so that we have all the labels together for a particular sentence (7.5 points)**\n",
    "\n",
    "   **a. Label columns to merge: “gender”, “age”, “topic”, “sign”**\n",
    "    \n",
    "   **b. After completing the previous step, there should be only two columns in your data frame i.e. “text” and “labels” as shown in the below image**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26aa0696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \n",
       "0  info found pages mb pdf files wait untill team...  \n",
       "1  team members drewes van der laag urllink mail ...  \n",
       "2  het kader van kernfusie op aarde maak je eigen...  \n",
       "3                                    testing testing  \n",
       "4  thanks yahoo toolbar capture urls popups means...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d40851b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['labels'] = df.apply(lambda row: [row['gender'],str(row['age']),row['topic'],row['sign']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b42173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>sign</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>14,May,2004</td>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>13,May,2004</td>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2059027</td>\n",
       "      <td>male</td>\n",
       "      <td>15</td>\n",
       "      <td>Student</td>\n",
       "      <td>Leo</td>\n",
       "      <td>12,May,2004</td>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3581210</td>\n",
       "      <td>male</td>\n",
       "      <td>33</td>\n",
       "      <td>InvestmentBanking</td>\n",
       "      <td>Aquarius</td>\n",
       "      <td>11,June,2004</td>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id gender  age              topic      sign          date  \\\n",
       "0  2059027   male   15            Student       Leo   14,May,2004   \n",
       "1  2059027   male   15            Student       Leo   13,May,2004   \n",
       "2  2059027   male   15            Student       Leo   12,May,2004   \n",
       "3  2059027   male   15            Student       Leo   12,May,2004   \n",
       "4  3581210   male   33  InvestmentBanking  Aquarius  11,June,2004   \n",
       "\n",
       "                                                text  \\\n",
       "0  info found pages mb pdf files wait untill team...   \n",
       "1  team members drewes van der laag urllink mail ...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups means...   \n",
       "\n",
       "                                    labels  \n",
       "0                 [male, 15, Student, Leo]  \n",
       "1                 [male, 15, Student, Leo]  \n",
       "2                 [male, 15, Student, Leo]  \n",
       "3                 [male, 15, Student, Leo]  \n",
       "4  [male, 33, InvestmentBanking, Aquarius]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad92ef2",
   "metadata": {},
   "source": [
    "Inference: the features 'gender','age','sign','topic' are combined under feature named 'labels' and these columns are excluded for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b86be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "678f91cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  info found pages mb pdf files wait untill team...   \n",
       "1  team members drewes van der laag urllink mail ...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups means...   \n",
       "\n",
       "                                    labels  \n",
       "0                 [male, 15, Student, Leo]  \n",
       "1                 [male, 15, Student, Leo]  \n",
       "2                 [male, 15, Student, Leo]  \n",
       "3                 [male, 15, Student, Leo]  \n",
       "4  [male, 33, InvestmentBanking, Aquarius]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4e228",
   "metadata": {},
   "source": [
    "**4. Separate features and labels, and split the data into training and testing (5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "886b6838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400,)\n",
      "(600,)\n",
      "(2400,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x = df.text.values\n",
    "y = df.labels.values\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,random_state=10,test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2175ba",
   "metadata": {},
   "source": [
    "**5. Vectorize the features (5 points)**\n",
    "\n",
    "**a. Create a Bag of Words using count vectorizer**\n",
    "\n",
    "i. Use ngram_range=(1, 2)\n",
    "\n",
    "ii. Vectorize training and testing features\n",
    "\n",
    "**b. Print the term-document matrix****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11245dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e94566d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toNumeric = CountVectorizer()\n",
    "toNumeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca9d8cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180518"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include 1 and 2-grams\n",
    "toNumeric = CountVectorizer(ngram_range=(1, 2),binary=True)\n",
    "toNumeric.fit(X_train)\n",
    "len(toNumeric.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29f49ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aa anger',\n",
       " 'aa compared',\n",
       " 'aa nice',\n",
       " 'aaa',\n",
       " 'aaa take',\n",
       " 'aaa travel',\n",
       " 'aaaaaah',\n",
       " 'aaaahh',\n",
       " 'aaagh',\n",
       " 'aaagh pero',\n",
       " 'aaarrrggghhhhhhhhgggghhhhhh',\n",
       " 'aaarrrggghhhhhhhhgggghhhhhh dropped',\n",
       " 'aal',\n",
       " 'aal eliminate',\n",
       " 'aal esseneth',\n",
       " 'aal lost',\n",
       " 'aaldering',\n",
       " 'aaldering urllink',\n",
       " 'aand',\n",
       " 'aand jim',\n",
       " 'aar',\n",
       " 'aar toy',\n",
       " 'aarde',\n",
       " 'aarde maak',\n",
       " 'aargh',\n",
       " 'aargh told',\n",
       " 'aaron',\n",
       " 'aaron burr',\n",
       " 'aaron rowand',\n",
       " 'aba',\n",
       " 'aba foundation',\n",
       " 'aba sessions',\n",
       " 'aba sus',\n",
       " 'aba therapy',\n",
       " 'aba well',\n",
       " 'abandon',\n",
       " 'abandon days',\n",
       " 'abandon every',\n",
       " 'abandon horses',\n",
       " 'abandon life',\n",
       " 'abandon may',\n",
       " 'abandoned',\n",
       " 'abandoned although',\n",
       " 'abandoned area',\n",
       " 'abandoning',\n",
       " 'abandoning isreal',\n",
       " 'abandons',\n",
       " 'abandons hate',\n",
       " 'abated',\n",
       " 'abated came',\n",
       " 'abbreviate',\n",
       " 'abbreviate fun',\n",
       " 'abc',\n",
       " 'abc last',\n",
       " 'abc nightline',\n",
       " 'abcnews',\n",
       " 'abcnews com',\n",
       " 'abcnews go',\n",
       " 'abdicate',\n",
       " 'abdicate overthrown',\n",
       " 'abdomen',\n",
       " 'abdomen thighs',\n",
       " 'abdominal',\n",
       " 'abdominal area',\n",
       " 'abercrombie',\n",
       " 'abercrombie boxers',\n",
       " 'aberrant',\n",
       " 'aberrant really',\n",
       " 'aberration',\n",
       " 'aberration got',\n",
       " 'abhor',\n",
       " 'abhor middle',\n",
       " 'abiding',\n",
       " 'abiding christian',\n",
       " 'abiding pedestrians',\n",
       " 'abilities',\n",
       " 'abilities documents',\n",
       " 'abilities rather',\n",
       " 'abilities really',\n",
       " 'ability',\n",
       " 'ability almost',\n",
       " 'ability asked',\n",
       " 'ability believe',\n",
       " 'ability edit',\n",
       " 'ability even',\n",
       " 'ability fire',\n",
       " 'ability jump',\n",
       " 'ability keep',\n",
       " 'ability outrun',\n",
       " 'ability practice',\n",
       " 'ability really',\n",
       " 'ability take',\n",
       " 'ability walk',\n",
       " 'abject',\n",
       " 'abject horror',\n",
       " 'ablaze',\n",
       " 'ablaze lot',\n",
       " 'able',\n",
       " 'able actually',\n",
       " 'able add',\n",
       " 'able answer',\n",
       " 'able anything',\n",
       " 'able broaden',\n",
       " 'able buy',\n",
       " 'able caddy',\n",
       " 'able catch',\n",
       " 'able come',\n",
       " 'able communicate',\n",
       " 'able conjure',\n",
       " 'able control',\n",
       " 'able convert',\n",
       " 'able depend',\n",
       " 'able dont',\n",
       " 'able drive',\n",
       " 'able engage',\n",
       " 'able every',\n",
       " 'able expensively',\n",
       " 'able face',\n",
       " 'able find',\n",
       " 'able fly',\n",
       " 'able fully',\n",
       " 'able fun',\n",
       " 'able function',\n",
       " 'able get',\n",
       " 'able give',\n",
       " 'able go',\n",
       " 'able golf',\n",
       " 'able hang',\n",
       " 'able hear',\n",
       " 'able help',\n",
       " 'able hit',\n",
       " 'able join',\n",
       " 'able learn',\n",
       " 'able least',\n",
       " 'able leave',\n",
       " 'able live',\n",
       " 'able look',\n",
       " 'able make',\n",
       " 'able master',\n",
       " 'able mile',\n",
       " 'able mix',\n",
       " 'able occasionally',\n",
       " 'able operate',\n",
       " 'able overcome',\n",
       " 'able post',\n",
       " 'able prove',\n",
       " 'able pull',\n",
       " 'able reach',\n",
       " 'able read',\n",
       " 'able recognise',\n",
       " 'able redefine',\n",
       " 'able resist',\n",
       " 'able ride',\n",
       " 'able said',\n",
       " 'able say',\n",
       " 'able separate',\n",
       " 'able sew',\n",
       " 'able shim',\n",
       " 'able show',\n",
       " 'able sing',\n",
       " 'able stand',\n",
       " 'able stick',\n",
       " 'able stroll',\n",
       " 'able take',\n",
       " 'able tell',\n",
       " 'able thing',\n",
       " 'able things',\n",
       " 'able ugh',\n",
       " 'able update',\n",
       " 'able urllink',\n",
       " 'able use',\n",
       " 'able utilize',\n",
       " 'able vote',\n",
       " 'able watch',\n",
       " 'able withstand',\n",
       " 'ablity',\n",
       " 'ablity teleport',\n",
       " 'abm',\n",
       " 'abm treaty',\n",
       " 'abnormal',\n",
       " 'abnormal screwed',\n",
       " 'abortion',\n",
       " 'abortion problem',\n",
       " 'abortions',\n",
       " 'abortions already',\n",
       " 'abortions must',\n",
       " 'abortions void',\n",
       " 'abounds',\n",
       " 'abounds sunshine',\n",
       " 'abraham',\n",
       " 'abraham lincoln',\n",
       " 'abreast',\n",
       " 'abreast current',\n",
       " 'abreast going',\n",
       " 'abreu',\n",
       " 'abreu phi',\n",
       " 'abri',\n",
       " 'abri el',\n",
       " 'abridged',\n",
       " 'abridged script',\n",
       " 'abridging',\n",
       " 'abridging freedom',\n",
       " 'abrio',\n",
       " 'abrio la',\n",
       " 'abroad',\n",
       " 'abroad attorney',\n",
       " 'abroad falls',\n",
       " 'abroad friends',\n",
       " 'abroad integrity',\n",
       " 'abroad position',\n",
       " 'abroad work',\n",
       " 'abrupt',\n",
       " 'abrupt click',\n",
       " 'abrupt times',\n",
       " 'abs',\n",
       " 'abs bland',\n",
       " 'abs favorite',\n",
       " 'abs maybe',\n",
       " 'abs spy',\n",
       " 'abscess',\n",
       " 'abscess bank',\n",
       " 'abscond',\n",
       " 'abscond need',\n",
       " 'absence',\n",
       " 'absence also',\n",
       " 'absence democratic',\n",
       " 'absence know',\n",
       " 'absence spam',\n",
       " 'absence specific',\n",
       " 'absent',\n",
       " 'absent bullies',\n",
       " 'absent character',\n",
       " 'absent could',\n",
       " 'absent show',\n",
       " 'absentee',\n",
       " 'absentee votes',\n",
       " 'absentia',\n",
       " 'absentia new',\n",
       " 'absolulty',\n",
       " 'absolulty past',\n",
       " 'absolute',\n",
       " 'absolute applies',\n",
       " 'absolute fairplay',\n",
       " 'absolute hoot',\n",
       " 'absolute last',\n",
       " 'absolute moxie',\n",
       " 'absolute opposition',\n",
       " 'absolute resources',\n",
       " 'absolutely',\n",
       " 'absolutely abhor',\n",
       " 'absolutely agree',\n",
       " 'absolutely beautiful',\n",
       " 'absolutely bubbles',\n",
       " 'absolutely called',\n",
       " 'absolutely certain',\n",
       " 'absolutely clue',\n",
       " 'absolutely confidence',\n",
       " 'absolutely convinced',\n",
       " 'absolutely could',\n",
       " 'absolutely dirty',\n",
       " 'absolutely disrespectful',\n",
       " 'absolutely english',\n",
       " 'absolutely fenomenal',\n",
       " 'absolutely hate',\n",
       " 'absolutely hates',\n",
       " 'absolutely hilarious',\n",
       " 'absolutely huge',\n",
       " 'absolutely idea',\n",
       " 'absolutely immediate',\n",
       " 'absolutely impossible',\n",
       " 'absolutely love',\n",
       " 'absolutely loveable',\n",
       " 'absolutely necessary',\n",
       " 'absolutely nice',\n",
       " 'absolutely nothing',\n",
       " 'absolutely pissed',\n",
       " 'absolutely refreshing',\n",
       " 'absolutely right',\n",
       " 'absolutely rocked',\n",
       " 'absolutely service',\n",
       " 'absolutely splash',\n",
       " 'absolutely stupid',\n",
       " 'absolutely ucalled',\n",
       " 'absolutes',\n",
       " 'absolutes always',\n",
       " 'absolutes none',\n",
       " 'absolutes objective',\n",
       " 'absolutes seems',\n",
       " 'absolutes wary',\n",
       " 'absolution',\n",
       " 'absolution couldnt',\n",
       " 'absolution many',\n",
       " 'absolution others',\n",
       " 'absolutist',\n",
       " 'absolutist taste',\n",
       " 'absolutistic',\n",
       " 'absolutly',\n",
       " 'absolutly courage',\n",
       " 'absolutly crazy',\n",
       " 'absolutly dont',\n",
       " 'absolutly nothing',\n",
       " 'absolutly pointless',\n",
       " 'absolving',\n",
       " 'absolving child',\n",
       " 'absolving parents',\n",
       " 'absorb',\n",
       " 'absorb cover',\n",
       " 'absorbed',\n",
       " 'absorbed give',\n",
       " 'absorbed residents',\n",
       " 'absorbed spookiness',\n",
       " 'absorbed thoughts',\n",
       " 'absorbent',\n",
       " 'absorbent cacophony',\n",
       " 'absorbing',\n",
       " 'absorbing panels',\n",
       " 'absoultly',\n",
       " 'absoultly love',\n",
       " 'abstain',\n",
       " 'abstain go',\n",
       " 'abstract',\n",
       " 'abstract art',\n",
       " 'abstract proposition',\n",
       " 'abstract true',\n",
       " 'abstraction',\n",
       " 'abstraction wonderful',\n",
       " 'abstractness',\n",
       " 'abstractness introversion',\n",
       " 'absurd',\n",
       " 'absurd explanation',\n",
       " 'absurd naive',\n",
       " 'absurdities',\n",
       " 'absurdities like',\n",
       " 'abt',\n",
       " 'abt week',\n",
       " 'abu',\n",
       " 'abu gharib',\n",
       " 'abu musab',\n",
       " 'abundance',\n",
       " 'abundance universe',\n",
       " 'abundant',\n",
       " 'abundant evidence',\n",
       " 'aburrici',\n",
       " 'aburrici nisiquiera',\n",
       " 'abuse',\n",
       " 'abuse ioc',\n",
       " 'abuse know',\n",
       " 'abused',\n",
       " 'abused serial',\n",
       " 'abusing',\n",
       " 'abusing eight',\n",
       " 'abusive',\n",
       " 'abusive childhood',\n",
       " 'abusive environment',\n",
       " 'abusive parents',\n",
       " 'abusive residents',\n",
       " 'ac',\n",
       " 'ac later',\n",
       " 'ac pre',\n",
       " 'ac unit',\n",
       " 'ac va',\n",
       " 'ac weekend',\n",
       " 'ac wp',\n",
       " 'aca',\n",
       " 'aca nos',\n",
       " 'aca tonces',\n",
       " 'academia',\n",
       " 'academia man',\n",
       " 'academics',\n",
       " 'academics adaptive',\n",
       " 'academies',\n",
       " 'academies family',\n",
       " 'academy',\n",
       " 'academy award',\n",
       " 'academy cannes',\n",
       " 'academy graduation',\n",
       " 'accent',\n",
       " 'accent apply',\n",
       " 'accent candy',\n",
       " 'accent courtesy',\n",
       " 'accent find',\n",
       " 'accent gaadnaa',\n",
       " 'accent got',\n",
       " 'accent knows',\n",
       " 'accent made',\n",
       " 'accent sentece',\n",
       " 'accent speaks',\n",
       " 'accent still',\n",
       " 'accent talking',\n",
       " 'accented',\n",
       " 'accented etc',\n",
       " 'accented voice',\n",
       " 'accents',\n",
       " 'accents french',\n",
       " 'accents would',\n",
       " 'accept',\n",
       " 'accept application',\n",
       " 'accept changed',\n",
       " 'accept choices',\n",
       " 'accept comes',\n",
       " 'accept delivery',\n",
       " 'accept deserve',\n",
       " 'accept especially',\n",
       " 'accept everything',\n",
       " 'accept fact',\n",
       " 'accept find',\n",
       " 'accept higher',\n",
       " 'accept jimi',\n",
       " 'accept known',\n",
       " 'accept laughter',\n",
       " 'accept monumental',\n",
       " 'accept move',\n",
       " 'accept reclaim',\n",
       " 'accept saw',\n",
       " 'accept script',\n",
       " 'accept suppose',\n",
       " 'accept things',\n",
       " 'accept us',\n",
       " 'accept wholeheartedly',\n",
       " 'acceptable',\n",
       " 'acceptable behavior',\n",
       " 'acceptable except',\n",
       " 'acceptable long',\n",
       " 'acceptable player',\n",
       " 'acceptable went',\n",
       " 'acceptable whats',\n",
       " 'acceptance',\n",
       " 'acceptance email',\n",
       " 'acceptance possibility',\n",
       " 'acceptance things',\n",
       " 'accepted',\n",
       " 'accepted broken',\n",
       " 'accepted chemistry',\n",
       " 'accepted english',\n",
       " 'accepted ethical',\n",
       " 'accepted fiction',\n",
       " 'accepted knows',\n",
       " 'accepted long',\n",
       " 'accepted marriage',\n",
       " 'accepted never',\n",
       " 'accepted offer',\n",
       " 'accepted phd',\n",
       " 'accepted prime',\n",
       " 'accepted think',\n",
       " 'accepting',\n",
       " 'accepting anything',\n",
       " 'accepting complements',\n",
       " 'access',\n",
       " 'access archives',\n",
       " 'access blog',\n",
       " 'access edit',\n",
       " 'access favorite',\n",
       " 'access flight',\n",
       " 'access forum',\n",
       " 'access get',\n",
       " 'access iraq',\n",
       " 'access last',\n",
       " 'access notes',\n",
       " 'access pot',\n",
       " 'access report',\n",
       " 'access server',\n",
       " 'access telephones',\n",
       " 'access vcr',\n",
       " 'accessibility',\n",
       " 'accessibility web',\n",
       " 'accessories',\n",
       " 'accessories etc',\n",
       " 'accessories plaid',\n",
       " 'accf',\n",
       " 'accf applause',\n",
       " 'accident',\n",
       " 'accident always',\n",
       " 'accident anyways',\n",
       " 'accident blonde',\n",
       " 'accident coupla',\n",
       " 'accident dying',\n",
       " 'accident east',\n",
       " 'accident even',\n",
       " 'accident everything',\n",
       " 'accident get',\n",
       " 'accident honest',\n",
       " 'accident last',\n",
       " 'accident mistake',\n",
       " 'accident number',\n",
       " 'accident physiology',\n",
       " 'accident tester',\n",
       " 'accident tree',\n",
       " 'accident watched',\n",
       " 'accident way',\n",
       " 'accidentally',\n",
       " 'accidentally broke',\n",
       " 'accidentally caused',\n",
       " 'accidentally destroy',\n",
       " 'accidentally hitting',\n",
       " 'accidentally ironing',\n",
       " 'accidentally slipped',\n",
       " 'accidentally willow',\n",
       " 'accidently',\n",
       " 'accidently blew',\n",
       " 'accidently destroyed',\n",
       " 'accidents',\n",
       " 'accidents charles',\n",
       " 'accidents point',\n",
       " 'accidents tickets',\n",
       " 'acclimate',\n",
       " 'acclimate shelter',\n",
       " 'acclimating',\n",
       " 'acclimating cyrillic',\n",
       " 'accolade',\n",
       " 'accolade give',\n",
       " 'accolades',\n",
       " 'accolades jennifer',\n",
       " 'accomadate',\n",
       " 'accomadate tattoo',\n",
       " 'accommodate',\n",
       " 'accommodate cough',\n",
       " 'accommodation',\n",
       " 'accommodation words',\n",
       " 'accomodate',\n",
       " 'accomodate false',\n",
       " 'accomodate pudginess',\n",
       " 'accompanied',\n",
       " 'accompanied cheesy',\n",
       " 'accompanied good',\n",
       " 'accompanied taylor',\n",
       " 'accomplices',\n",
       " 'accomplices defeated',\n",
       " 'accomplices taking',\n",
       " 'accomplish',\n",
       " 'accomplish feat',\n",
       " 'accomplish grace',\n",
       " 'accomplish stress',\n",
       " 'accomplish task',\n",
       " 'accomplish think',\n",
       " 'accomplish work',\n",
       " 'accomplished',\n",
       " 'accomplished country',\n",
       " 'accomplished evaluate',\n",
       " 'accomplished little',\n",
       " 'accomplished nothing',\n",
       " 'accomplished reply',\n",
       " 'accomplished spooning',\n",
       " 'accomplished track',\n",
       " 'accomplished truly',\n",
       " 'accomplished trust',\n",
       " 'accomplishment',\n",
       " 'accomplishment following',\n",
       " 'accomplishment susan',\n",
       " 'accomplishments',\n",
       " 'accomplishments exploded',\n",
       " 'accomplishments let',\n",
       " 'accomplishments probably',\n",
       " 'accomplishments provide',\n",
       " 'accomplishments results',\n",
       " 'accomplishments skills',\n",
       " 'accomplishments telling',\n",
       " 'accomplishments want',\n",
       " 'accord',\n",
       " 'accord air',\n",
       " 'accord going',\n",
       " 'accord making',\n",
       " 'according',\n",
       " 'according amount',\n",
       " 'according article',\n",
       " 'according beacon',\n",
       " 'according beat',\n",
       " 'according bigger',\n",
       " 'according black',\n",
       " 'according camp',\n",
       " 'according castillion',\n",
       " 'according cbs',\n",
       " 'according constitution',\n",
       " 'according court',\n",
       " 'according cursory',\n",
       " 'according dad',\n",
       " 'according einstein',\n",
       " 'according electoral',\n",
       " 'according enemies',\n",
       " 'according enough',\n",
       " 'according federal',\n",
       " 'according friend',\n",
       " 'according individual',\n",
       " 'according jeffrey',\n",
       " 'according joint',\n",
       " 'according memorandum',\n",
       " 'according miss',\n",
       " 'according moore',\n",
       " 'according one',\n",
       " 'according phil',\n",
       " 'according plan',\n",
       " 'according postal',\n",
       " 'according publicly',\n",
       " 'according red',\n",
       " 'according rolling',\n",
       " 'according spoilers',\n",
       " 'according study',\n",
       " 'according surrounding',\n",
       " 'according tastes',\n",
       " 'according time',\n",
       " 'according urllink',\n",
       " 'according woman',\n",
       " 'accordingly',\n",
       " 'accordingly constitutes',\n",
       " 'accordingly court',\n",
       " 'accordingly heck',\n",
       " 'account',\n",
       " 'account activated',\n",
       " 'account almost',\n",
       " 'account americas',\n",
       " 'account anyway',\n",
       " 'account anyways',\n",
       " 'account block',\n",
       " 'account brian',\n",
       " 'account cancelled',\n",
       " 'account crisis',\n",
       " 'account days',\n",
       " 'account dictionary',\n",
       " 'account drove',\n",
       " 'account earn',\n",
       " 'account forwarded',\n",
       " 'account grand',\n",
       " 'account jason',\n",
       " 'account karma',\n",
       " 'account last',\n",
       " 'account local',\n",
       " 'account logon',\n",
       " 'account make',\n",
       " 'account means',\n",
       " 'account need',\n",
       " 'account nifty',\n",
       " 'account old',\n",
       " 'account pic',\n",
       " 'account referring',\n",
       " 'account said',\n",
       " 'account sat',\n",
       " 'account savings',\n",
       " 'account send',\n",
       " 'account though',\n",
       " 'account time',\n",
       " 'accountable',\n",
       " 'accountable distortion',\n",
       " 'accountable http',\n",
       " 'accountable wrongful',\n",
       " 'accountant',\n",
       " 'accountant also',\n",
       " 'accountant paper',\n",
       " 'accountant son',\n",
       " 'accountant taxpayer',\n",
       " 'accountants',\n",
       " 'accountants tidying',\n",
       " 'accounted',\n",
       " 'accounted grieve',\n",
       " 'accounted taxes',\n",
       " 'accounted thanks',\n",
       " 'accounting',\n",
       " 'accounting software',\n",
       " 'accounts',\n",
       " 'accounts deaths',\n",
       " 'accounts easier',\n",
       " 'accounts former',\n",
       " 'accounts gets',\n",
       " 'accounts glossed',\n",
       " 'accounts history',\n",
       " 'accounts macs',\n",
       " 'accounts reason',\n",
       " 'accounts supplement',\n",
       " 'accoustic',\n",
       " 'accoustic life',\n",
       " 'accpetance',\n",
       " 'accpetance mail',\n",
       " 'accross',\n",
       " 'accross floor',\n",
       " 'accrue',\n",
       " 'accrue taxes',\n",
       " 'accudent',\n",
       " 'accudent later',\n",
       " 'accumulated',\n",
       " 'accumulated credit',\n",
       " 'accumulated two',\n",
       " 'accumulated wasted',\n",
       " 'accuracy',\n",
       " 'accuracy depending',\n",
       " 'accuracy information',\n",
       " 'accuracy mean',\n",
       " 'accuracy one',\n",
       " 'accurate',\n",
       " 'accurate definition',\n",
       " 'accurate far',\n",
       " 'accurate find',\n",
       " 'accurate make',\n",
       " 'accurate numbers',\n",
       " 'accurate party',\n",
       " 'accurately',\n",
       " 'accurately pathetic',\n",
       " 'accusations',\n",
       " 'accusations seem',\n",
       " 'accused',\n",
       " 'accused believe',\n",
       " 'accused embezzling',\n",
       " 'accused huh',\n",
       " 'accused julius',\n",
       " 'accused spoiler',\n",
       " 'accused stretching',\n",
       " 'accuses',\n",
       " 'accuses justice',\n",
       " 'accusing',\n",
       " 'accusing got',\n",
       " 'accusing mr',\n",
       " 'ace',\n",
       " 'ace ass',\n",
       " 'ace bandages',\n",
       " 'acepto',\n",
       " 'acepto se',\n",
       " 'acerco',\n",
       " 'acerco la',\n",
       " 'ache',\n",
       " 'ache ache',\n",
       " 'ache bled',\n",
       " 'ache get',\n",
       " 'ache pain',\n",
       " 'ache really',\n",
       " 'ache tell',\n",
       " 'acheive',\n",
       " 'acheive aims',\n",
       " 'aches',\n",
       " 'aches early',\n",
       " 'aches horrible',\n",
       " 'aches nausea',\n",
       " 'aches smell',\n",
       " 'achieve',\n",
       " 'achieve healing',\n",
       " 'achieve something',\n",
       " 'achieve unless',\n",
       " 'achieved',\n",
       " 'achieved goal',\n",
       " 'achieved program',\n",
       " 'achieved simply',\n",
       " 'achieved success',\n",
       " 'achieved variety',\n",
       " 'achievement',\n",
       " 'achievement grateful',\n",
       " 'achievements',\n",
       " 'achievements made',\n",
       " 'aching',\n",
       " 'aching feet',\n",
       " 'aching move',\n",
       " 'aching tired',\n",
       " 'achtung',\n",
       " 'achtung ichli',\n",
       " 'acid',\n",
       " 'acid bite',\n",
       " 'acid chickens',\n",
       " 'acid corrosive',\n",
       " 'acid free',\n",
       " 'acid rain',\n",
       " 'acid uranium',\n",
       " 'ack',\n",
       " 'ack anyway',\n",
       " 'ack cursed',\n",
       " 'ack easily',\n",
       " 'ack feeling',\n",
       " 'ack going',\n",
       " 'ack hate',\n",
       " 'ack microsoft',\n",
       " 'ack plus',\n",
       " 'ack sort',\n",
       " 'ack weird',\n",
       " 'acknowledge',\n",
       " 'acknowledge always',\n",
       " 'acknowledge sounds',\n",
       " 'acknowledged',\n",
       " 'acknowledged bigots',\n",
       " 'acknowledged course',\n",
       " 'acknowledged mostly',\n",
       " 'acknowledged suv',\n",
       " 'acknowledged terrorist',\n",
       " 'acknowledgement',\n",
       " 'acknowledgement scifi',\n",
       " 'acknowledgement someone',\n",
       " 'acknowledging',\n",
       " 'acknowledging concerns',\n",
       " 'acknowledging farce',\n",
       " 'acknowledging mutual',\n",
       " 'acknowledging one',\n",
       " 'acland',\n",
       " 'acland street',\n",
       " 'aclu',\n",
       " 'aclu getting',\n",
       " 'aclu kids',\n",
       " 'aclu org',\n",
       " 'aclu would',\n",
       " 'acne',\n",
       " 'acne skin',\n",
       " 'acoustic',\n",
       " 'acoustic singalongs',\n",
       " 'acquaintances',\n",
       " 'acquaintances best',\n",
       " 'acquaintances eons',\n",
       " 'acquainted',\n",
       " 'acquainted basically',\n",
       " 'acquire',\n",
       " 'acquire antidote',\n",
       " 'acquire infantry',\n",
       " 'acquired',\n",
       " 'acquired bottle',\n",
       " 'acquired enriched',\n",
       " 'acquired popcorn',\n",
       " 'acquired proof',\n",
       " 'acquired stuffed',\n",
       " 'acquisition',\n",
       " 'acquisition certain',\n",
       " 'acquisition etc',\n",
       " 'acquisitions',\n",
       " 'acquisitions clothes',\n",
       " 'acres',\n",
       " 'acres cost',\n",
       " 'acres land',\n",
       " 'across',\n",
       " 'across abnormal',\n",
       " 'across ad',\n",
       " 'across addendum',\n",
       " 'across aisle',\n",
       " 'across america',\n",
       " 'across board',\n",
       " 'across bridge',\n",
       " 'across country',\n",
       " 'across daily',\n",
       " 'across day',\n",
       " 'across desert',\n",
       " 'across disconcerting',\n",
       " 'across europe',\n",
       " 'across face',\n",
       " 'across front',\n",
       " 'across ground',\n",
       " 'across hall',\n",
       " 'across insted',\n",
       " 'across interesting',\n",
       " 'across jump',\n",
       " 'across land',\n",
       " 'across lawn',\n",
       " 'across length',\n",
       " 'across little',\n",
       " 'across maybe',\n",
       " 'across message',\n",
       " 'across middle',\n",
       " 'across miserable',\n",
       " 'across notebook',\n",
       " 'across ocean',\n",
       " 'across painted',\n",
       " 'across picture',\n",
       " 'across piece',\n",
       " 'across plexiglas',\n",
       " 'across pool',\n",
       " 'across reading',\n",
       " 'across road',\n",
       " 'across room',\n",
       " 'across somewhat',\n",
       " 'across street',\n",
       " 'across stuff',\n",
       " 'across table',\n",
       " 'across theatre',\n",
       " 'across town',\n",
       " 'across train',\n",
       " 'across uneven',\n",
       " 'across urllink',\n",
       " 'across via',\n",
       " 'across water',\n",
       " 'across waters',\n",
       " 'across website',\n",
       " 'across yard',\n",
       " 'across years',\n",
       " 'acrosse',\n",
       " 'acrosse table',\n",
       " 'acs',\n",
       " 'acs mgs',\n",
       " 'act',\n",
       " 'act abandon',\n",
       " 'act among',\n",
       " 'act anyway',\n",
       " 'act around',\n",
       " 'act balancing',\n",
       " 'act become',\n",
       " 'act broke',\n",
       " 'act certain',\n",
       " 'act consider',\n",
       " 'act cute',\n",
       " 'act dead',\n",
       " 'act differently',\n",
       " 'act disconnect',\n",
       " 'act etc',\n",
       " 'act first',\n",
       " 'act general',\n",
       " 'act get',\n",
       " 'act ghetto',\n",
       " 'act good',\n",
       " 'act hatred',\n",
       " 'act interests',\n",
       " 'act like',\n",
       " 'act listening',\n",
       " 'act loneliness',\n",
       " 'act much',\n",
       " 'act obtain',\n",
       " 'act ordinary',\n",
       " 'act porn',\n",
       " 'act righteous',\n",
       " 'act satisfy',\n",
       " 'act see',\n",
       " 'act sexism',\n",
       " 'act situation',\n",
       " 'act take',\n",
       " 'act though',\n",
       " 'act thought',\n",
       " 'act together',\n",
       " 'act tough',\n",
       " 'act truth',\n",
       " 'act voluntarily',\n",
       " 'act want',\n",
       " 'act watch',\n",
       " 'act way',\n",
       " 'act wonder',\n",
       " 'act yesterday',\n",
       " 'actaully',\n",
       " 'actaully gave',\n",
       " 'acted',\n",
       " 'acted heard',\n",
       " 'acted unprofessoinal',\n",
       " 'acted way',\n",
       " 'acting',\n",
       " 'acting although',\n",
       " 'acting btw',\n",
       " 'acting character',\n",
       " 'acting choppy',\n",
       " 'acting cute',\n",
       " 'acting ever',\n",
       " 'acting funny',\n",
       " 'acting good',\n",
       " 'acting lifestyle',\n",
       " 'acting like',\n",
       " 'acting modeling',\n",
       " 'acting performance',\n",
       " 'acting premature',\n",
       " 'acting sparkles',\n",
       " 'acting trying',\n",
       " 'acting white',\n",
       " 'acting works',\n",
       " 'acting young',\n",
       " 'action',\n",
       " 'action action',\n",
       " 'action arranged',\n",
       " 'action current',\n",
       " 'action extravaganza',\n",
       " 'action films',\n",
       " 'action firestone',\n",
       " 'action hand',\n",
       " 'action initial',\n",
       " 'action iraq',\n",
       " 'action joan',\n",
       " 'action jovial',\n",
       " 'action killing',\n",
       " 'action little',\n",
       " 'action longer',\n",
       " 'action made',\n",
       " 'action master',\n",
       " 'action necessary',\n",
       " 'action need',\n",
       " 'action needs',\n",
       " 'action others',\n",
       " 'action photos',\n",
       " 'action really',\n",
       " 'action saddest',\n",
       " 'action scenes',\n",
       " 'action see',\n",
       " 'action sports',\n",
       " 'action start',\n",
       " 'action suggesting',\n",
       " 'action suit',\n",
       " 'action well',\n",
       " 'action without',\n",
       " 'action yay',\n",
       " 'action yeah',\n",
       " 'action year',\n",
       " 'action young',\n",
       " 'actions',\n",
       " 'actions air',\n",
       " 'actions bush',\n",
       " 'actions crime',\n",
       " 'actions dont',\n",
       " 'actions effects',\n",
       " 'actions endanger',\n",
       " 'actions may',\n",
       " 'actions ok',\n",
       " 'actions perform',\n",
       " 'actions society',\n",
       " 'actions terrorists',\n",
       " 'actions toward',\n",
       " 'activated',\n",
       " 'activated scares',\n",
       " 'activated th',\n",
       " 'activated way',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toNumeric.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e129b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dtm = toNumeric.fit_transform(X_train)\n",
    "X_test_dtm = toNumeric.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6464d0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'aa anger', 'aa compared', 'aa nice', 'aaa']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toNumeric.get_feature_names()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e39d5a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b59c286b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787ec02",
   "metadata": {},
   "source": [
    "**6. Create a dictionary to get the count of every label i.e. the key will be label name and value will be the\n",
    "total count of the label. Check below image for reference (5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2a8af24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>info found pages mb pdf files wait untill team...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>team members drewes van der laag urllink mail ...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>het kader van kernfusie op aarde maak je eigen...</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing testing</td>\n",
       "      <td>[male, 15, Student, Leo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thanks yahoo toolbar capture urls popups means...</td>\n",
       "      <td>[male, 33, InvestmentBanking, Aquarius]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  info found pages mb pdf files wait untill team...   \n",
       "1  team members drewes van der laag urllink mail ...   \n",
       "2  het kader van kernfusie op aarde maak je eigen...   \n",
       "3                                    testing testing   \n",
       "4  thanks yahoo toolbar capture urls popups means...   \n",
       "\n",
       "                                    labels  \n",
       "0                 [male, 15, Student, Leo]  \n",
       "1                 [male, 15, Student, Leo]  \n",
       "2                 [male, 15, Student, Leo]  \n",
       "3                 [male, 15, Student, Leo]  \n",
       "4  [male, 33, InvestmentBanking, Aquarius]  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d01144a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = dict()\n",
    "\n",
    "for labels in df.labels.values:\n",
    "    for label in labels:\n",
    "        if label in label_counts:\n",
    "            label_counts[label] += 1\n",
    "        else:\n",
    "            label_counts[label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d1fedc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'male': 2272,\n",
       " '15': 299,\n",
       " 'Student': 403,\n",
       " 'Leo': 55,\n",
       " '33': 94,\n",
       " 'InvestmentBanking': 70,\n",
       " 'Aquarius': 286,\n",
       " 'female': 728,\n",
       " '14': 74,\n",
       " 'indUnk': 452,\n",
       " 'Aries': 1699,\n",
       " '25': 110,\n",
       " 'Capricorn': 77,\n",
       " '17': 147,\n",
       " 'Gemini': 21,\n",
       " '23': 93,\n",
       " 'Non-Profit': 46,\n",
       " 'Cancer': 76,\n",
       " 'Banking': 16,\n",
       " '37': 19,\n",
       " 'Sagittarius': 113,\n",
       " '26': 43,\n",
       " '24': 334,\n",
       " 'Scorpio': 243,\n",
       " '27': 86,\n",
       " 'Education': 118,\n",
       " '45': 14,\n",
       " 'Engineering': 119,\n",
       " 'Libra': 313,\n",
       " 'Science': 33,\n",
       " '34': 6,\n",
       " '41': 14,\n",
       " 'Communications-Media': 14,\n",
       " 'BusinessServices': 21,\n",
       " 'Sports-Recreation': 75,\n",
       " 'Virgo': 39,\n",
       " 'Taurus': 76,\n",
       " 'Arts': 2,\n",
       " 'Pisces': 2,\n",
       " '44': 3,\n",
       " '16': 25,\n",
       " 'Internet': 20,\n",
       " 'Museums-Libraries': 2,\n",
       " 'Accounting': 2,\n",
       " '39': 32,\n",
       " '35': 1607,\n",
       " 'Technology': 1607}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31d4a1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['male', '15', 'Student', 'Leo', '33', 'InvestmentBanking', 'Aquarius', 'female', '14', 'indUnk', 'Aries', '25', 'Capricorn', '17', 'Gemini', '23', 'Non-Profit', 'Cancer', 'Banking', '37', 'Sagittarius', '26', '24', 'Scorpio', '27', 'Education', '45', 'Engineering', 'Libra', 'Science', '34', '41', 'Communications-Media', 'BusinessServices', 'Sports-Recreation', 'Virgo', 'Taurus', 'Arts', 'Pisces', '44', '16', 'Internet', 'Museums-Libraries', 'Accounting', '39', '35', 'Technology'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf4771",
   "metadata": {},
   "source": [
    "**7. Transform the labels - (7.5 points)**\n",
    "\n",
    "As we have noticed before, in this task each example can have multiple tags. To deal with such kind of\n",
    "prediction, we need to transform labels in a binary form and the prediction will be a mask of 0s and 1s.\n",
    "For this purpose, it is convenient to use MultiLabelBinarizer from sklearn\n",
    "\n",
    "a. Convert your train and test labels using MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e3557d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95426139",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(label_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c5ce40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e358b862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14', '15', '16', '17', '23', '24', '25', '26', '27', '33', '34',\n",
       "       '35', '37', '39', '41', '44', '45', 'Accounting', 'Aquarius',\n",
       "       'Aries', 'Arts', 'Banking', 'BusinessServices', 'Cancer',\n",
       "       'Capricorn', 'Communications-Media', 'Education', 'Engineering',\n",
       "       'Gemini', 'Internet', 'InvestmentBanking', 'Leo', 'Libra',\n",
       "       'Museums-Libraries', 'Non-Profit', 'Pisces', 'Sagittarius',\n",
       "       'Science', 'Scorpio', 'Sports-Recreation', 'Student', 'Taurus',\n",
       "       'Technology', 'Virgo', 'female', 'indUnk', 'male'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac5e3c",
   "metadata": {},
   "source": [
    "**8. Choose a classifier - (5 points)**\n",
    "\n",
    "In this task, we suggest using the One-vs-Rest approach, which is implemented in OneVsRestClassifier\n",
    "class. In this approach k classifiers (= number of tags) are trained. As a basic classifier, use\n",
    "LogisticRegression. It is one of the simplest methods, but often it performs good enough in text\n",
    "classification tasks. It might take some time because the number of classifiers to train is large.\n",
    "\n",
    "a. Use a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label\n",
    "\n",
    "b. As One-vs-Rest approach might not have been discussed in the sessions, we are providing you\n",
    "with the code for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cdd8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='lbfgs')\n",
    "clf = OneVsRestClassifier(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc399799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression())"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e31fbd",
   "metadata": {},
   "source": [
    "**9. Fit the classifier, make predictions and get the accuracy (5 points)**\n",
    "\n",
    "a. Print the following\n",
    "\n",
    "i. Accuracy score\n",
    "\n",
    "ii. F1 score\n",
    "\n",
    "iii. Average precision score\n",
    "\n",
    "iv. Average recall score\n",
    "\n",
    "v. Tip: Make sure you are familiar with all of them. How would you expect the things to\n",
    "work for the multi-label scenario? Read about micro/macro/weighted averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0952675",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60683152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a6b365a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "y_test_inversed = mlb.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae0af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\tlove food network encompasses iron chef rules enjoy almost anything run e law order third watch murder wrote magnum pi l law also adore st elsewhere think show bravo love matlock really deal diagnosis murder tips cheesy scales much also love remington steele scarecrow mrs king bruce boxleitner sp pierce brosnan double dose hot course columbo rocks love man annoys suspects confessing also really loved cheesy mountie show due south rerunning tnt sure think right jesus watch way much television\n",
      "True labels:\t35,Aries,Technology,male\n",
      "Predicted labels:\t35,Aries,Technology,male\n",
      "\n",
      "\n",
      "Title:\talpha omega returned alpha camp weekend thoroughly enjoyable pretty far melbie though upper plenty far hardly melway map eep place fantastic dorm rooms toilet really neat clean esp bathroom amenities needed great food plus real fireplace could ask spent n enjoying company learning holy spirit part alpha course taking church sorta like foundation back basics christianity kind thing real good stuff eg great time probably coz serious male bonding going bunch like minded noisy guys sitting around lunch talking cars n stuff blokey made friend alan promise never turn guys sitting around congratulating masters universe said guarantees sigh happened sensitive new age guys one highlight learning mafia really interesting game someone gets killed gotta find think murderer meets bluff meets survivor get gist mafiosi doctor spy rest civilians people plus game master see heated debates everyone tried defend prevent thought compadres voted layered game never know bluffing fun also interesting coz got group christians essentially lying hmmm hehe anyways great experience definitely chance right slogging n trying keep essay schedule cheerios\n",
      "True labels:\t24,Scorpio,female,indUnk\n",
      "Predicted labels:\t24,Scorpio,female,indUnk\n",
      "\n",
      "\n",
      "Title:\tsanti santi dear sweet child live help unfortunate merfolk like poor souls one else turn admit past nasty kidding called well witch find nowadays mended ways repented seen light made switch true yes fortunately know little magic talent always possessed lately please laugh use behalf miserable lonely depressed pathetic poor unfortunate souls pain need one longing thinner one wants get girl help yes indeed poor unfortunate souls sad true come flocking cauldron crying spells ursula please help yes happened twice someone pay price afraid rake em cross coals yes odd complaint whole saint poor unfortunate souls come poor unfortunate soul go ahead make choice busy woman got day cost much voice poor unfortunate soul sad true want cross bridge sweet got pay toll take gulp take breath go ahead sign scroll poor unfortunate soul little mermaid\n",
      "True labels:\t15,Aquarius,Student,female\n",
      "Predicted labels:\tmale\n",
      "\n",
      "\n",
      "Title:\treport urllink espn com today says ricky williams would failed drug test would played year serving game suspension means tested positive twice check pro pot quotes fyi ricky williams nbsp long bout depression diagonsed social anxiety disorder nbsp takes paxil marijuana plant nbsp nbsp marijuana times better paxil said dolphins head coach dave wannstedt knew nothing wannstedt said totally surprised shocked apparently actually urllink gotten away smoking weed job last two years drinking masking agent fools scientists urine tests forgot take masking nbsp supplement year nbsp year round drug nbsp tests dolphins suprised guy smokes pot nbsp friends lenny kravitz got dreadlocks admires life bob marley nbsp nbsp ever watched guy lunch line nbsp nbsp masking agent used fool nfl ever since retirement given tons free publicity urine luck nbsp one greatest names product ever nbsp\n",
      "True labels:\t23,Taurus,indUnk,male\n",
      "Predicted labels:\tindUnk,male\n",
      "\n",
      "\n",
      "Title:\tembarking hour time way flinders mornington peninsula area overnight camp retreat intensive rest community going equipped tools teaching bonds friendship weekend carry us tide turmoil everchanging world got frantic sms mum today read disbelievingly called back immediately hear distress voice sounded like crying rightly suppose found closest aunt dad side declared hate word brain dead brain dead almost cliche like sufficiently melodramatic phenomena constantly portrayed daytime soap opera channel serial life support right full details suffice say shocked idea coming cancer remission awhile never known hospital last couple months sudden bringing name always conjured images love warmth fantastic food always enjoy place cny though ever saw trips home lunar new year period maintained certain sense closeness feel anticipated meeting sometime trip back december even cny possible wedding april know feel close yet distant worried anxious helpless yet quite believing quite coming terms maybe mum message wrong maybe might brain dead kinda thing mum often leans towards theatrics run family wait see maybe wrong hopefully prayerfully uncomprehendingly\n",
      "True labels:\t24,Scorpio,female,indUnk\n",
      "Predicted labels:\tfemale\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n",
    "        X_test[i],\n",
    "        ','.join(y_test_inversed[i]),\n",
    "        ','.join(y_pred_inversed[i])\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10bd2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def print_evaluation_scores(y_val, predicted):\n",
    "    print('Accuracy score: ', accuracy_score(y_val, predicted))\n",
    "    print('F1 score: ', f1_score(y_val, predicted, average='micro'))\n",
    "    print('Average precision score: ', average_precision_score(y_val, predicted, average='micro'))\n",
    "    print('Average recall score: ', recall_score(y_val, predicted, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f80d26f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.54\n",
      "F1 score:  0.735632183908046\n",
      "Average precision score:  0.579383667766852\n",
      "Average recall score:  0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5104a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "173e63cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.07      0.12        15\n",
      "           1       0.96      0.32      0.48        68\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       1.00      0.23      0.37        35\n",
      "           4       1.00      0.07      0.13        14\n",
      "           5       0.92      0.40      0.56        60\n",
      "           6       0.00      0.00      0.00        30\n",
      "           7       0.00      0.00      0.00        11\n",
      "           8       0.00      0.00      0.00        11\n",
      "           9       1.00      0.25      0.40        24\n",
      "          10       0.00      0.00      0.00         2\n",
      "          11       0.84      0.94      0.89       309\n",
      "          12       0.00      0.00      0.00         3\n",
      "          13       0.00      0.00      0.00         4\n",
      "          14       0.00      0.00      0.00         4\n",
      "          15       0.00      0.00      0.00         0\n",
      "          16       0.00      0.00      0.00         1\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.93      0.22      0.36        58\n",
      "          19       0.83      0.92      0.87       330\n",
      "          20       0.00      0.00      0.00         1\n",
      "          21       0.00      0.00      0.00         4\n",
      "          22       1.00      0.20      0.33         5\n",
      "          23       1.00      0.05      0.10        19\n",
      "          24       1.00      0.41      0.58        17\n",
      "          25       0.00      0.00      0.00         4\n",
      "          26       0.00      0.00      0.00        15\n",
      "          27       1.00      0.31      0.48        16\n",
      "          28       0.00      0.00      0.00         4\n",
      "          29       0.00      0.00      0.00         6\n",
      "          30       1.00      0.33      0.50        18\n",
      "          31       0.00      0.00      0.00        13\n",
      "          32       0.82      0.31      0.45        58\n",
      "          33       0.00      0.00      0.00         2\n",
      "          34       0.00      0.00      0.00        11\n",
      "          35       0.00      0.00      0.00         1\n",
      "          36       1.00      0.08      0.15        24\n",
      "          37       0.00      0.00      0.00         9\n",
      "          38       0.92      0.24      0.38        50\n",
      "          39       1.00      0.44      0.61        16\n",
      "          40       0.81      0.23      0.36        90\n",
      "          41       0.00      0.00      0.00        19\n",
      "          42       0.84      0.94      0.89       309\n",
      "          43       0.00      0.00      0.00         7\n",
      "          44       0.77      0.47      0.58       149\n",
      "          45       0.89      0.33      0.48        94\n",
      "          46       0.84      0.95      0.90       451\n",
      "\n",
      "   micro avg       0.84      0.65      0.74      2400\n",
      "   macro avg       0.45      0.19      0.23      2400\n",
      "weighted avg       0.80      0.65      0.67      2400\n",
      " samples avg       0.81      0.65      0.69      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89e85026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def display_metrics_micro(y_val, predicted):\n",
    "    print('Accuracy score: ', accuracy_score(y_val, predicted))\n",
    "    print('F1 score: Micro', f1_score(y_val, predicted, average='micro'))\n",
    "    print('Average precision score: Micro', average_precision_score(y_val, predicted, average='micro'))\n",
    "    print('Average recall score: Micro', recall_score(y_val, predicted, average='micro'))\n",
    "    \n",
    "    \n",
    "def display_metrics_macro(y_val, predicted):\n",
    "    print('Accuracy score: ', accuracy_score(y_val, predicted))\n",
    "    print('F1 score: Macro', f1_score(y_val, predicted, average='macro'))\n",
    "    print('Average recall score: MAcro', recall_score(y_val, predicted, average='macro'))\n",
    "    \n",
    "def display_metrics_weighted(y_val, predicted):\n",
    "    print('Accuracy score: ', accuracy_score(y_val, predicted))\n",
    "    print('F1 score: weighted', f1_score(y_val, predicted, average='weighted'))\n",
    "    print('Average precision score: weighted', average_precision_score(y_val, predicted, average='weighted'))\n",
    "    print('Average recall score: weighted', recall_score(y_val, predicted, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c32f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.54\n",
      "F1 score: Micro 0.735632183908046\n",
      "Average precision score: Micro 0.579383667766852\n",
      "Average recall score: Micro 0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "display_metrics_micro(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f167284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.54\n",
      "F1 score: Macro 0.23362146939402045\n",
      "Average recall score: MAcro 0.18562280828959574\n"
     ]
    }
   ],
   "source": [
    "display_metrics_macro(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af5dc3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.54\n",
      "F1 score: weighted 0.6686151046086233\n",
      "Average precision score: weighted 0.6013572937816467\n",
      "Average recall score: weighted 0.6533333333333333\n"
     ]
    }
   ],
   "source": [
    "display_metrics_weighted(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4078a",
   "metadata": {},
   "source": [
    "**10. Print true label and predicted label for any five examples (7.5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2060719f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('35', 'Aries', 'Technology', 'male'),\n",
       " ('24', 'Scorpio', 'female', 'indUnk'),\n",
       " ('15', 'Aquarius', 'Student', 'female'),\n",
       " ('23', 'Taurus', 'indUnk', 'male'),\n",
       " ('24', 'Scorpio', 'female', 'indUnk')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_inversed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69b114e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('35', 'Aries', 'Technology', 'male'),\n",
       " ('24', 'Scorpio', 'female', 'indUnk'),\n",
       " ('male',),\n",
       " ('indUnk', 'male'),\n",
       " ('female',)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_inversed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "751fbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def print_predicted(y_predicted, y_test = y_test , n = 5):\n",
    "    j = []\n",
    "    for i in range(n):\n",
    "        j.append(random.randint(0, len(y_test)))\n",
    "    print(j)\n",
    "                 \n",
    "    for k in j:\n",
    "        print(mlb.inverse_transform(y_predicted)[k])\n",
    "        print(mlb.inverse_transform(y_test)[k])\n",
    "        print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3fba2871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95, 449, 13, 369, 270]\n",
      "('35', 'Aries', 'Technology', 'male')\n",
      "('35', 'Aries', 'Technology', 'male')\n",
      "-----------------------------------\n",
      "('female', 'indUnk')\n",
      "('24', 'Scorpio', 'female', 'indUnk')\n",
      "-----------------------------------\n",
      "('15', 'Student', 'female')\n",
      "('15', 'Aquarius', 'Student', 'male')\n",
      "-----------------------------------\n",
      "('male',)\n",
      "('17', 'Sagittarius', 'Student', 'male')\n",
      "-----------------------------------\n",
      "('male',)\n",
      "('41', 'Communications-Media', 'Libra', 'male')\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_predicted(y_predicted=y_pred, y_test = y_test , n = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
